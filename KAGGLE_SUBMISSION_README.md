# ğŸ¯ KAGGLE SUBMISSION - Enhanced Model

## ğŸ“Š Performance Summary

| Model | Validation SMAPE | Description |
|-------|------------------|-------------|
| **Baseline** | 30.70% | Tabular features only (LightGBM) |
| **Enhanced** | **27.86%** | Tabular + Text PCA (20 components) |
| **Improvement** | **+2.84%** | 9.2% relative improvement |

**Expected Leaderboard:** 27-30% SMAPE

---

## ğŸ—ï¸ Model Architecture

### **Features (42 total):**

**Tabular Features (22):**
- IPQ: `value`, `unit`, `pack_size`
- Quality: `has_premium`, `has_organic`, `has_gourmet`, `has_natural`, `has_artisan`, `has_luxury`
- Size: `is_travel_size`, `is_bulk`
- Brand: `brand`, `brand_exists`
- Interactions: 
  - ValueÃ—Quality: `value_x_premium`, `value_x_luxury`, `value_x_organic`
  - PackÃ—Quality: `pack_x_premium`, `pack_x_value`
  - BrandÃ—Quality: `brand_x_premium`, `brand_x_organic`
  - SizeÃ—Value: `travel_x_value`, `bulk_x_value`

**Text PCA Features (20):**
- Top 20 PCA components from CLIP text embeddings (ViT-B/32)
- Explains ~60% of embedding variance
- Captures semantic price signals

### **Algorithm:**
- **Model:** LightGBM (GBDT)
- **Transform:** Square root on target
- **Training:** Full training data (75,000 samples)
- **Hyperparameters:**
  - `num_leaves`: 63
  - `learning_rate`: 0.03
  - `num_boost_round`: 2000
  - `lambda_l1/l2`: 0.5

---

## ğŸ“ˆ Performance by Segment

| Segment | Validation SMAPE | % of Data |
|---------|------------------|-----------|
| **Mid-Range** ($10-$50) | **19.01%** âœ…âœ… | 50.9% |
| **Premium** ($50-$100) | **32.32%** âœ… | 8.3% |
| **Budget** (<$10) | **37.44%** âš ï¸ | 38.4% |
| **Luxury** (>$100) | **46.14%** âš ï¸ | 2.5% |

**Strong on mid-range, struggles with budget/luxury extremes**

---

## ğŸ¯ Key Insights

### **1. Text Embeddings ARE Useful (When Used Correctly)**

**Wrong approach (Direct regression):**
```
Text embeddings only â†’ 60.26% SMAPE âŒ
```

**Right approach (PCA + Tabular):**
```
Text PCA + Tabular â†’ 27.86% SMAPE âœ…
```

### **2. Feature Importance:**

**Top 10 Features:**
1. ğŸ“Š `value` (6,074 importance)
2. ğŸ“ `text_pca_11` (5,426)
3. ğŸ“ `text_pca_10` (5,372)
4. ğŸ“ `text_pca_15` (5,326)
5. ğŸ“ `text_pca_1` (5,234)
6. ğŸ“ `text_pca_16` (5,233)
7. ğŸ“ `text_pca_2` (5,177)
8. ğŸ“ `text_pca_12` (5,145)
9. ğŸ“ `text_pca_13` (5,125)
10. ğŸ“ `text_pca_0` (5,121)

**9/10 top features are text PCA** - embeddings dominate!

**Importance by category:**
- Text PCA: 81.1%
- Tabular: 18.9%

### **3. Image Embeddings: NOT Used**

**Analysis showed:**
- Image CLIP embeddings: RÂ² = -0.0085 (worse than baseline)
- CLIP not trained for product price prediction from images
- Would need fine-tuning (8+ hours, high risk)
- **Decision:** Skip images, focus on text + tabular

---

## ğŸ“ Files for Kaggle

### **Upload to Kaggle:**
1. `KAGGLE_submission_enhanced.py` - Main script
2. `train_text_embeddings_clip.npy` - Pre-generated (outputs/)
3. `test_text_embeddings_clip.npy` - Pre-generated (outputs/)
4. `train_with_features.csv` - Engineered features (dataset/)
5. `test_with_features.csv` - Engineered features (dataset/)

### **Run on Kaggle:**
```python
python /kaggle/working/amazon-ml-chal/KAGGLE_submission_enhanced.py
```

**Output:** `submission_enhanced.csv`

---

## ğŸ”§ How to Generate Test Features

If `test_with_features.csv` doesn't exist, run:

```python
python generate_features.py
```

This will extract the same 22 tabular features from test set.

---

## ğŸ“Š Validation Results

**5-Fold Cross-Validation:**
- Fold 1: 28.35%
- Fold 2: 27.81%
- Fold 3: 27.77%
- Fold 4: 27.66%
- Fold 5: 27.72%

**Mean:** 27.86%  
**Std:** 0.25% (very stable!)

---

## ğŸ’¡ Why This Works

### **1. Engineered Features Capture Structure:**
- `value Ã— pack_size` â†’ quantity pricing
- `has_premium Ã— value` â†’ premium larger packs
- `brand` â†’ manufacturer positioning

### **2. Text PCA Captures Semantics:**
- Product descriptions â†’ quality signals
- "organic", "gourmet", "luxury" keywords
- Category information (coffee vs water)

### **3. SQRT Transform Balances Ranges:**
- Less aggressive than log for cheap items
- Better for wide price range ($0.13 - $2,796)
- Improves mid-range prediction (50% of data)

---

## ğŸš€ Next Steps (Optional Improvements)

### **Ensemble (Expected: 26-28% SMAPE):**
```python
# Train multiple models
- LightGBM (current)
- CatBoost  
- XGBoost

# Average predictions
final = 0.4*lgb + 0.3*cat + 0.3*xgb
```

### **More PCA Components (Test 30-50):**
```python
# Current: 20 components (60% variance)
# Try: 30-50 components (70-80% variance)
# Risk: May overfit
```

### **Log Transform (Alternative):**
```python
# Current: SQRT
# Try: log1p (might help luxury segment)
```

---

## ğŸ“ Submission Checklist

- [x] Trained on full training data (75,000 samples)
- [x] PCA fitted on train, transformed test
- [x] Label encoders handle unseen categories
- [x] Predictions clipped to positive values
- [x] Inverse transform applied (sqrtÂ² )
- [x] Submission format: id, price
- [x] Sanity checks: reasonable distribution

---

## ğŸ¯ Expected Results

**Validation:** 27.86% SMAPE  
**Expected LB:** 27-30% SMAPE (Â±2% variance)

**If LB is:**
- **<28%:** âœ…âœ… Excellent! Model generalizes well
- **28-30%:** âœ… Good! As expected
- **30-32%:** âš ï¸ Some overfitting, but acceptable
- **>32%:** âŒ Investigate: test distribution mismatch?

---

## ğŸ”¬ What Didn't Work

1. **Image CLIP Embeddings:** RÂ² = -0.01 (completely useless)
2. **Direct Text Regression:** 60% SMAPE (too weak alone)
3. **Random Forest:** Overfits to leaked features
4. **Log Transform:** Worse than SQRT for this data

---

## ğŸ‘ Credits

**Approach:** Incremental feature validation  
**Baseline:** 30.70% SMAPE (tabular only)  
**Enhancement:** +Text PCA â†’ 27.86% SMAPE  

**Key Learning:** Embeddings work best as supplementary features, not standalone!
